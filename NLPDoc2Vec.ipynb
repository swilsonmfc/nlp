{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPDoc2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNz+Vz9ZnruCEwzRY5kcB3v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swilsonmfc/nlp/blob/master/NLPDoc2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC3uaWkdjQTF",
        "colab_type": "text"
      },
      "source": [
        "# Document Representations\n",
        "![](https://songsforteaching.net/wp-content/uploads/2019/01/wordcloud-lyrics-1.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsGLFnwMjV_1",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBvG_SaLoqCm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "35ea6e9e-4d00-48be-c8b8-1c53d6b32e79"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhrAQFd9QPrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import glob\n",
        "import random\n",
        "import collections\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import RandomSampler\n",
        "from torch.utils.data import SequentialSampler\n",
        "\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "from gensim.similarities.docsim import Similarity\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification\n",
        "from transformers import AdamW\n",
        "from transformers import BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import ndcg_score"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDFVr0abPrBt",
        "colab_type": "text"
      },
      "source": [
        "# Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuhS26l9vLba",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "d8cd514f-f2a0-4288-f230-c51e1f4cd27e"
      },
      "source": [
        "!pip install kaggle\n",
        "!mkdir -p ~/.kaggle"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.6.20)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBgCJawsvQnC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du54xlsZvQlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv kaggle.json ~/.kaggle\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvJWX1eajWM-",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rxOoy2WQNb8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c4cdd7b1-79ea-4859-f462-ec8a19bb3280"
      },
      "source": [
        "!kaggle datasets download -d paultimothymooney/poetry"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading poetry.zip to /content\n",
            "\r  0% 0.00/2.00M [00:00<?, ?B/s]\n",
            "\r100% 2.00M/2.00M [00:00<00:00, 140MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EodeaADrSJl9",
        "colab_type": "text"
      },
      "source": [
        "## Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nK_H4B2Q4ZZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c62b556-b0a7-414c-dd3d-c36f4520afde"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "poetry.zip  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnJTYaOuQ6LJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q poetry.zip"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICN6AlFKQ9Aj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm Kanye_West.txt kanye.txt notorious_big.txt Lil_Wayne.txt"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3LCcNH1SPgQ",
        "colab_type": "text"
      },
      "source": [
        "## Artist 2 Lyrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKNJOJSVRCy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "artist2lyrics = {}\n",
        "artist2id = {}\n",
        "id2artist = []\n",
        "\n",
        "counter = 0\n",
        "for file in glob.glob('*.txt'):\n",
        "  artist = file[:-4]\n",
        "  with open(file, 'r') as f:\n",
        "    lyrics = f.read()\n",
        "\n",
        "  artist2lyrics[artist] = lyrics\n",
        "  artist2id[artist] = counter\n",
        "  id2artist.append(artist)\n",
        "  counter += 1"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnKAzv3BSCq2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "1d55bcbc-412c-4bd7-9c98-ee102551742e"
      },
      "source": [
        "artist2lyrics['blink-182'][0:500]"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Can we forget about the things I said when I was drunk...\\nI didn't mean to call you that\\nI can't remember what was said\\nOr what you threw at me Please tell me\\nPlease tell me why\\nMy car is in the front yard\\nAnd I am sleeping with my cloths on\\nI came in throught the window... Last night\\nAnd your... Gone\\nGone It's no suprise to me I am my own worst enemy\\nCuz every now and then I kick the living shit out of me\\nThe smoke alarm is going offf and there a cigarette\\nStill buring Please tell me why\\nMy car\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGya6Y5jcB5K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "1e562b07-9ef1-4776-9dc5-c85ab4efda04"
      },
      "source": [
        "artist2id"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'adele': 22,\n",
              " 'al-green': 23,\n",
              " 'alicia-keys': 4,\n",
              " 'amy-winehouse': 9,\n",
              " 'beatles': 41,\n",
              " 'bieber': 28,\n",
              " 'bjork': 33,\n",
              " 'blink-182': 19,\n",
              " 'bob-dylan': 1,\n",
              " 'bob-marley': 31,\n",
              " 'britney-spears': 11,\n",
              " 'bruce-springsteen': 40,\n",
              " 'bruno-mars': 14,\n",
              " 'cake': 36,\n",
              " 'dickinson': 26,\n",
              " 'disney': 37,\n",
              " 'dj-khaled': 7,\n",
              " 'dolly-parton': 2,\n",
              " 'dr-seuss': 18,\n",
              " 'drake': 43,\n",
              " 'eminem': 8,\n",
              " 'janisjoplin': 39,\n",
              " 'jimi-hendrix': 3,\n",
              " 'johnny-cash': 25,\n",
              " 'joni-mitchell': 21,\n",
              " 'kanye-west': 5,\n",
              " 'lady-gaga': 30,\n",
              " 'leonard-cohen': 38,\n",
              " 'lil-wayne': 35,\n",
              " 'lin-manuel-miranda': 17,\n",
              " 'lorde': 12,\n",
              " 'ludacris': 10,\n",
              " 'michael-jackson': 32,\n",
              " 'missy-elliott': 15,\n",
              " 'nickelback': 20,\n",
              " 'nicki-minaj': 44,\n",
              " 'nirvana': 0,\n",
              " 'notorious-big': 42,\n",
              " 'nursery_rhymes': 16,\n",
              " 'patti-smith': 24,\n",
              " 'paul-simon': 27,\n",
              " 'prince': 34,\n",
              " 'r-kelly': 29,\n",
              " 'radiohead': 13,\n",
              " 'rihanna': 6}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biw4MdMOORRA",
        "colab_type": "text"
      },
      "source": [
        "# Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6iglFY1Zsn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_corpus(artist_lyrics, tokens_only=False):\n",
        "  for artist, lyrics in artist_lyrics.items():\n",
        "      tokens = gensim.utils.simple_preprocess(lyrics)\n",
        "      artist_id = artist2id[artist]\n",
        "\n",
        "      if tokens_only:\n",
        "          yield tokens\n",
        "      else:\n",
        "          # For training data, add tags\n",
        "          yield gensim.models.doc2vec.TaggedDocument(tokens, [artist_id])\n",
        "\n",
        "\n",
        "def split_train_test_corpus(corpus, split=0.75):\n",
        "  train = []\n",
        "  test  = []\n",
        "\n",
        "  for words, artist_id in corpus:\n",
        "    train_len = int(len(words) * split)\n",
        "    train_c = words[0: train_len]\n",
        "    test_c  = words[train_len: ]\n",
        "\n",
        "    train.append(gensim.models.doc2vec.TaggedDocument(train_c, artist_id))\n",
        "    test.append(gensim.models.doc2vec.TaggedDocument(test_c, artist_id))\n",
        "  \n",
        "  return train, test\n",
        "\n",
        "corpus = list(read_corpus(artist2lyrics))\n",
        "train_corpus, test_corpus = split_train_test_corpus(corpus)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJmxeY4vagmU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "b00bae5a-d4fa-4b4e-a7e5-989c7a99d3c7"
      },
      "source": [
        "# Nirvana\n",
        "train_corpus[0].words[0:20]"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['load',\n",
              " 'up',\n",
              " 'on',\n",
              " 'guns',\n",
              " 'bring',\n",
              " 'your',\n",
              " 'friends',\n",
              " 'it',\n",
              " 'fun',\n",
              " 'to',\n",
              " 'lose',\n",
              " 'and',\n",
              " 'to',\n",
              " 'pretend',\n",
              " 'she',\n",
              " 'over',\n",
              " 'bored',\n",
              " 'and',\n",
              " 'self',\n",
              " 'assured']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOEuiyyRRhqo",
        "colab_type": "text"
      },
      "source": [
        "# Topic Models\n",
        "* We could look to Topic Modeling\n",
        "* Apply LSA or LDA \n",
        "* Settle on number of topics using Coherence metric\n",
        "* Map documents to LSA / LDA space (vectorized document)\n",
        "* Compare similarity "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JC3-gwUuIBOn",
        "colab_type": "text"
      },
      "source": [
        "# Word2Vec\n",
        "* Approach to map a word to a low dimensional space\n",
        "* Build using a language model and word co-occurence (predict words)\n",
        "* Implemented using Skip-Gram or Continuous Bag of Words\n",
        "\n",
        "![Img](https://www.researchgate.net/profile/Nailah_Al-Madi/publication/319954363/figure/fig1/AS:552189871353858@1508663732919/CBOW-and-Skip-gram-models-architecture-1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnWMMZbycOQA",
        "colab_type": "text"
      },
      "source": [
        "## Embedding Algebra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhIP7o55cAxR",
        "colab_type": "text"
      },
      "source": [
        "![](https://www.researchgate.net/profile/Joshua_Peterson2/publication/316921211/figure/fig2/AS:667626124619787@1536185880504/The-parallelogram-model-of-analogy-completes-the-analogy-king-queen-man-by.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mngqq6mBnSMK",
        "colab_type": "text"
      },
      "source": [
        "## Load\n",
        "* We'll pull global vectors built from wiki\n",
        "* This embedding represents words in 50 dimensions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5x8Z5m6LYQV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "09cf8f9b-fa46-4172-f7e6-9a2284fc32f0"
      },
      "source": [
        "wv = api.load('glove-wiki-gigaword-50')"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MsEB3yIn3LD",
        "colab_type": "text"
      },
      "source": [
        "## GloVe\n",
        "* GloVe came out of Stanford (https://nlp.stanford.edu/pubs/glove.pdf)\n",
        "* Researched what is Word2Vec optimizing\n",
        "* Dramatic improvement in run time\n",
        "* Similar to Word2Vec\n",
        "  * Captures co-occurrence statistics\n",
        "  * Distance between embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8yiFsHNJNYl",
        "colab_type": "text"
      },
      "source": [
        "# Mean of Word Embeddings\n",
        "* Look up the embedding for each word in the document's corpus\n",
        "* Average the unique embeddings in the document\n",
        "* Builds on the algebraic properties of embeddings in word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKWejbEGQUVs",
        "colab_type": "text"
      },
      "source": [
        "## MOWE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Btx8lBb8K6xC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_mowe(corpus):\n",
        "  mowe = []\n",
        "\n",
        "  for doc in corpus:\n",
        "    bag = set()\n",
        "    words = 0\n",
        "    vec = None\n",
        "\n",
        "    for word in doc.words:\n",
        "      if word in bag:\n",
        "        continue\n",
        "      bag.add(word)\n",
        "\n",
        "      if word in wv.vocab:\n",
        "        vec = wv.word_vec(word) if vec is None else vec + wv.word_vec(word)\n",
        "        words += 1\n",
        "\n",
        "    vec = vec / words\n",
        "    mowe.append(vec)\n",
        "  \n",
        "  return mowe\n",
        "\n",
        "train_mowe = compute_mowe(train_corpus)\n",
        "test_mowe  = compute_mowe(test_corpus)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkSSRltcixl3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "cbfad651-a01d-43ff-8e92-43691d6b631b"
      },
      "source": [
        "train_mowe[0]"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.54950336e-01,  4.86932844e-02, -5.89837367e-03, -2.91883439e-01,\n",
              "        2.88334399e-01,  6.17414564e-02, -2.34456003e-01,  9.05345753e-02,\n",
              "       -7.93985054e-02,  1.10978894e-01, -7.67452344e-02,  1.35282323e-01,\n",
              "       -1.06742002e-01,  4.55242582e-02,  3.58889312e-01,  2.27107227e-01,\n",
              "        5.71474433e-02,  1.24719357e-02, -1.33265048e-01, -3.96098226e-01,\n",
              "       -9.29171667e-02,  2.28529528e-01,  3.03278029e-01,  3.97048220e-02,\n",
              "        2.74321079e-01, -1.06808078e+00, -3.62022132e-01,  2.75569260e-01,\n",
              "        4.85836953e-01, -3.33712518e-01,  2.06391025e+00,  2.67924607e-01,\n",
              "       -1.57810912e-01, -2.93849725e-02, -8.08097050e-02,  5.92822805e-02,\n",
              "        3.92161235e-02,  1.40430881e-02,  1.35458097e-01, -2.66414702e-01,\n",
              "       -1.17865518e-01,  1.59786525e-03, -3.20789032e-02,  2.25130722e-01,\n",
              "        1.91300213e-01,  2.20919233e-02, -1.29248267e-02, -2.55970389e-01,\n",
              "       -1.10096335e-02, -2.80430745e-02], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf8r2F6vQQmL",
        "colab_type": "text"
      },
      "source": [
        "## Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWs99jzHf-po",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cb6bbe44-3286-4a4f-b498-03abe43baf3b"
      },
      "source": [
        "results = []\n",
        "\n",
        "counter = 0\n",
        "for test_doc in test_mowe:\n",
        "  test_doc = test_mowe[counter].reshape(1, -1)\n",
        "\n",
        "  artist_sim = []\n",
        "  for train_doc in train_mowe:\n",
        "    sim = cosine_similarity(train_doc.reshape(1, -1), test_doc).squeeze()\n",
        "    artist_sim.append(sim)  \n",
        "  \n",
        "  max = np.argmax(artist_sim)\n",
        "  min = np.argmin(artist_sim)\n",
        "  results.append({\n",
        "      'artist' : id2artist[counter],\n",
        "      'most_similar' : id2artist[max],\n",
        "      'most': np.max(artist_sim),\n",
        "      'least_similar' : id2artist[min],\n",
        "      'least': np.min(artist_sim)\n",
        "  })\n",
        "  counter += 1\n",
        "\n",
        "frame = pd.DataFrame(results)\n",
        "print('Matches', np.sum(np.where(frame.artist == frame.most_similar, 1, 0)))\n",
        "frame"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matches 24\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>artist</th>\n",
              "      <th>most_similar</th>\n",
              "      <th>most</th>\n",
              "      <th>least_similar</th>\n",
              "      <th>least</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>nirvana</td>\n",
              "      <td>nirvana</td>\n",
              "      <td>0.997396</td>\n",
              "      <td>missy-elliott</td>\n",
              "      <td>0.954433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bob-dylan</td>\n",
              "      <td>dolly-parton</td>\n",
              "      <td>0.996203</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.961341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dolly-parton</td>\n",
              "      <td>dolly-parton</td>\n",
              "      <td>0.998775</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.950177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>jimi-hendrix</td>\n",
              "      <td>janisjoplin</td>\n",
              "      <td>0.997687</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.939707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>alicia-keys</td>\n",
              "      <td>al-green</td>\n",
              "      <td>0.998158</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.942114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>kanye-west</td>\n",
              "      <td>kanye-west</td>\n",
              "      <td>0.997076</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.924841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>rihanna</td>\n",
              "      <td>janisjoplin</td>\n",
              "      <td>0.997364</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.939276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>dj-khaled</td>\n",
              "      <td>dj-khaled</td>\n",
              "      <td>0.997465</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.935030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>eminem</td>\n",
              "      <td>eminem</td>\n",
              "      <td>0.998251</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.946674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>amy-winehouse</td>\n",
              "      <td>amy-winehouse</td>\n",
              "      <td>0.998092</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.946748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>ludacris</td>\n",
              "      <td>lil-wayne</td>\n",
              "      <td>0.998706</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.936131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>britney-spears</td>\n",
              "      <td>janisjoplin</td>\n",
              "      <td>0.997482</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.946250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>lorde</td>\n",
              "      <td>lorde</td>\n",
              "      <td>0.997481</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.948548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>radiohead</td>\n",
              "      <td>nirvana</td>\n",
              "      <td>0.996184</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.956444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>bruno-mars</td>\n",
              "      <td>bruno-mars</td>\n",
              "      <td>0.993583</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.899167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>missy-elliott</td>\n",
              "      <td>nicki-minaj</td>\n",
              "      <td>0.997429</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.924309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>nursery_rhymes</td>\n",
              "      <td>nursery_rhymes</td>\n",
              "      <td>0.991898</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.922916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>lin-manuel-miranda</td>\n",
              "      <td>lin-manuel-miranda</td>\n",
              "      <td>0.996609</td>\n",
              "      <td>missy-elliott</td>\n",
              "      <td>0.915357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>dr-seuss</td>\n",
              "      <td>beatles</td>\n",
              "      <td>0.992659</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.920755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>blink-182</td>\n",
              "      <td>blink-182</td>\n",
              "      <td>0.998157</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.949433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>nickelback</td>\n",
              "      <td>nickelback</td>\n",
              "      <td>0.997151</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.941702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>joni-mitchell</td>\n",
              "      <td>joni-mitchell</td>\n",
              "      <td>0.997951</td>\n",
              "      <td>missy-elliott</td>\n",
              "      <td>0.959521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>adele</td>\n",
              "      <td>adele</td>\n",
              "      <td>0.997932</td>\n",
              "      <td>missy-elliott</td>\n",
              "      <td>0.947952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>al-green</td>\n",
              "      <td>al-green</td>\n",
              "      <td>0.997530</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.937299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>patti-smith</td>\n",
              "      <td>paul-simon</td>\n",
              "      <td>0.996169</td>\n",
              "      <td>missy-elliott</td>\n",
              "      <td>0.946835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>johnny-cash</td>\n",
              "      <td>johnny-cash</td>\n",
              "      <td>0.995299</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.935158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>dickinson</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.992140</td>\n",
              "      <td>missy-elliott</td>\n",
              "      <td>0.929531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>paul-simon</td>\n",
              "      <td>paul-simon</td>\n",
              "      <td>0.997805</td>\n",
              "      <td>missy-elliott</td>\n",
              "      <td>0.949648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>bieber</td>\n",
              "      <td>al-green</td>\n",
              "      <td>0.997808</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.938821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>r-kelly</td>\n",
              "      <td>janisjoplin</td>\n",
              "      <td>0.997492</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.940692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>lady-gaga</td>\n",
              "      <td>r-kelly</td>\n",
              "      <td>0.996190</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.947124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>bob-marley</td>\n",
              "      <td>bob-marley</td>\n",
              "      <td>0.996492</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.934539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>michael-jackson</td>\n",
              "      <td>alicia-keys</td>\n",
              "      <td>0.995898</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.936080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>bjork</td>\n",
              "      <td>al-green</td>\n",
              "      <td>0.993587</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.945686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>prince</td>\n",
              "      <td>prince</td>\n",
              "      <td>0.998304</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.949587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>lil-wayne</td>\n",
              "      <td>dj-khaled</td>\n",
              "      <td>0.996685</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.933734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>cake</td>\n",
              "      <td>blink-182</td>\n",
              "      <td>0.995750</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.953599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>disney</td>\n",
              "      <td>dolly-parton</td>\n",
              "      <td>0.994817</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.948293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>leonard-cohen</td>\n",
              "      <td>paul-simon</td>\n",
              "      <td>0.995279</td>\n",
              "      <td>missy-elliott</td>\n",
              "      <td>0.944784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>janisjoplin</td>\n",
              "      <td>janisjoplin</td>\n",
              "      <td>0.996715</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.920778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>bruce-springsteen</td>\n",
              "      <td>bruce-springsteen</td>\n",
              "      <td>0.995634</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.938621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>beatles</td>\n",
              "      <td>janisjoplin</td>\n",
              "      <td>0.997262</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.931601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>notorious-big</td>\n",
              "      <td>lil-wayne</td>\n",
              "      <td>0.998318</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.935270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>drake</td>\n",
              "      <td>drake</td>\n",
              "      <td>0.998378</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.935049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>nicki-minaj</td>\n",
              "      <td>nicki-minaj</td>\n",
              "      <td>0.997915</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.923757</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                artist        most_similar      most  least_similar     least\n",
              "0              nirvana             nirvana  0.997396  missy-elliott  0.954433\n",
              "1            bob-dylan        dolly-parton  0.996203      dickinson  0.961341\n",
              "2         dolly-parton        dolly-parton  0.998775      dickinson  0.950177\n",
              "3         jimi-hendrix         janisjoplin  0.997687      dickinson  0.939707\n",
              "4          alicia-keys            al-green  0.998158      dickinson  0.942114\n",
              "5           kanye-west          kanye-west  0.997076      dickinson  0.924841\n",
              "6              rihanna         janisjoplin  0.997364      dickinson  0.939276\n",
              "7            dj-khaled           dj-khaled  0.997465      dickinson  0.935030\n",
              "8               eminem              eminem  0.998251      dickinson  0.946674\n",
              "9        amy-winehouse       amy-winehouse  0.998092      dickinson  0.946748\n",
              "10            ludacris           lil-wayne  0.998706      dickinson  0.936131\n",
              "11      britney-spears         janisjoplin  0.997482      dickinson  0.946250\n",
              "12               lorde               lorde  0.997481      dickinson  0.948548\n",
              "13           radiohead             nirvana  0.996184      dickinson  0.956444\n",
              "14          bruno-mars          bruno-mars  0.993583      dickinson  0.899167\n",
              "15       missy-elliott         nicki-minaj  0.997429      dickinson  0.924309\n",
              "16      nursery_rhymes      nursery_rhymes  0.991898      dickinson  0.922916\n",
              "17  lin-manuel-miranda  lin-manuel-miranda  0.996609  missy-elliott  0.915357\n",
              "18            dr-seuss             beatles  0.992659      dickinson  0.920755\n",
              "19           blink-182           blink-182  0.998157      dickinson  0.949433\n",
              "20          nickelback          nickelback  0.997151      dickinson  0.941702\n",
              "21       joni-mitchell       joni-mitchell  0.997951  missy-elliott  0.959521\n",
              "22               adele               adele  0.997932  missy-elliott  0.947952\n",
              "23            al-green            al-green  0.997530      dickinson  0.937299\n",
              "24         patti-smith          paul-simon  0.996169  missy-elliott  0.946835\n",
              "25         johnny-cash         johnny-cash  0.995299      dickinson  0.935158\n",
              "26           dickinson           dickinson  0.992140  missy-elliott  0.929531\n",
              "27          paul-simon          paul-simon  0.997805  missy-elliott  0.949648\n",
              "28              bieber            al-green  0.997808      dickinson  0.938821\n",
              "29             r-kelly         janisjoplin  0.997492      dickinson  0.940692\n",
              "30           lady-gaga             r-kelly  0.996190      dickinson  0.947124\n",
              "31          bob-marley          bob-marley  0.996492      dickinson  0.934539\n",
              "32     michael-jackson         alicia-keys  0.995898      dickinson  0.936080\n",
              "33               bjork            al-green  0.993587      dickinson  0.945686\n",
              "34              prince              prince  0.998304      dickinson  0.949587\n",
              "35           lil-wayne           dj-khaled  0.996685      dickinson  0.933734\n",
              "36                cake           blink-182  0.995750      dickinson  0.953599\n",
              "37              disney        dolly-parton  0.994817      dickinson  0.948293\n",
              "38       leonard-cohen          paul-simon  0.995279  missy-elliott  0.944784\n",
              "39         janisjoplin         janisjoplin  0.996715      dickinson  0.920778\n",
              "40   bruce-springsteen   bruce-springsteen  0.995634      dickinson  0.938621\n",
              "41             beatles         janisjoplin  0.997262      dickinson  0.931601\n",
              "42       notorious-big           lil-wayne  0.998318      dickinson  0.935270\n",
              "43               drake               drake  0.998378      dickinson  0.935049\n",
              "44         nicki-minaj         nicki-minaj  0.997915      dickinson  0.923757"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNlt-9gqJNjT",
        "colab_type": "text"
      },
      "source": [
        "# Weighted MOWE\n",
        "* A weighted average approach builds on a simple average based on bag of words\n",
        "* We take the frequencies of the words in the document to weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBKrzkT9k9AO",
        "colab_type": "text"
      },
      "source": [
        "## W-MOWE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1UZPLr4RCoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_weighted_mowe(corpus):\n",
        "  mowe = []\n",
        "\n",
        "  for doc in corpus:\n",
        "    vec = None\n",
        "    words = 0\n",
        "\n",
        "    for word in doc.words:\n",
        "      if word in wv.vocab:\n",
        "        vec = wv.word_vec(word) if vec is None else vec + wv.word_vec(word)\n",
        "        words += 1\n",
        "\n",
        "    vec = vec / words\n",
        "    mowe.append(vec)\n",
        "  \n",
        "  return mowe\n",
        "\n",
        "train_weighted_mowe = compute_weighted_mowe(train_corpus)\n",
        "test_weighted_mowe  = compute_weighted_mowe(test_corpus)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2qLi2Ymkwhw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b0e01d28-f29f-473c-f9b1-0bbefdfb528e"
      },
      "source": [
        "train_weighted_mowe[0]"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.19559497,  0.11724761,  0.00607274, -0.26764312,  0.37272996,\n",
              "       -0.00441005, -0.37493387,  0.06237694, -0.21506894,  0.08326995,\n",
              "       -0.07717003,  0.25325766, -0.2991491 , -0.05970439,  0.5339159 ,\n",
              "        0.28429908,  0.1188302 ,  0.03791129, -0.1660163 , -0.47083682,\n",
              "       -0.1320999 ,  0.32936293,  0.36215812,  0.08034883,  0.359624  ,\n",
              "       -1.5455637 , -0.4903922 ,  0.1924811 ,  0.47487378, -0.53221357,\n",
              "        2.87099   ,  0.3474461 , -0.3326021 , -0.12064427, -0.04965992,\n",
              "       -0.04459094,  0.107208  ,  0.05076714,  0.18876286, -0.28145605,\n",
              "       -0.1556599 ,  0.07327984, -0.05377859,  0.20016786,  0.10080919,\n",
              "        0.02889267, -0.07625623, -0.26759976, -0.04125334,  0.1480381 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6bwuVMxk6rW",
        "colab_type": "text"
      },
      "source": [
        "## Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR-U8AnRjwKJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "25c1fbb3-9870-4467-a9b1-3de9d897b3c3"
      },
      "source": [
        "results = []\n",
        "\n",
        "counter = 0\n",
        "for test_doc in test_weighted_mowe:\n",
        "  test_doc = test_weighted_mowe[counter].reshape(1, -1)\n",
        "\n",
        "  artist_sim = []\n",
        "  for train_doc in train_weighted_mowe:\n",
        "    sim = cosine_similarity(train_doc.reshape(1, -1), test_doc).squeeze()\n",
        "    artist_sim.append(sim)  \n",
        "  \n",
        "  max = np.argmax(artist_sim)\n",
        "  min = np.argmin(artist_sim)\n",
        "  results.append({\n",
        "      'artist' : id2artist[counter],\n",
        "      'most_similar' : id2artist[max],\n",
        "      'most': np.max(artist_sim),\n",
        "      'least_similar' : id2artist[min],\n",
        "      'least': np.min(artist_sim)\n",
        "  })\n",
        "  counter += 1\n",
        "\n",
        "frame = pd.DataFrame(results)\n",
        "print('Matches', np.sum(np.where(frame.artist == frame.most_similar, 1, 0)))\n",
        "frame"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matches 30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>artist</th>\n",
              "      <th>most_similar</th>\n",
              "      <th>most</th>\n",
              "      <th>least_similar</th>\n",
              "      <th>least</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>nirvana</td>\n",
              "      <td>eminem</td>\n",
              "      <td>0.998748</td>\n",
              "      <td>nursery_rhymes</td>\n",
              "      <td>0.982941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bob-dylan</td>\n",
              "      <td>bob-dylan</td>\n",
              "      <td>0.999597</td>\n",
              "      <td>nursery_rhymes</td>\n",
              "      <td>0.988499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dolly-parton</td>\n",
              "      <td>jimi-hendrix</td>\n",
              "      <td>0.999089</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.981122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>jimi-hendrix</td>\n",
              "      <td>r-kelly</td>\n",
              "      <td>0.999065</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.976016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>alicia-keys</td>\n",
              "      <td>bieber</td>\n",
              "      <td>0.999369</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.966633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>kanye-west</td>\n",
              "      <td>kanye-west</td>\n",
              "      <td>0.999645</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.974730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>rihanna</td>\n",
              "      <td>michael-jackson</td>\n",
              "      <td>0.999473</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.970357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>dj-khaled</td>\n",
              "      <td>dj-khaled</td>\n",
              "      <td>0.999566</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.975715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>eminem</td>\n",
              "      <td>eminem</td>\n",
              "      <td>0.999473</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.981217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>amy-winehouse</td>\n",
              "      <td>amy-winehouse</td>\n",
              "      <td>0.999179</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.979417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>ludacris</td>\n",
              "      <td>ludacris</td>\n",
              "      <td>0.999674</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.975157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>britney-spears</td>\n",
              "      <td>britney-spears</td>\n",
              "      <td>0.999612</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.965700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>lorde</td>\n",
              "      <td>lorde</td>\n",
              "      <td>0.998750</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.979298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>radiohead</td>\n",
              "      <td>cake</td>\n",
              "      <td>0.998910</td>\n",
              "      <td>nursery_rhymes</td>\n",
              "      <td>0.985800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>bruno-mars</td>\n",
              "      <td>bruno-mars</td>\n",
              "      <td>0.997774</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.957522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>missy-elliott</td>\n",
              "      <td>missy-elliott</td>\n",
              "      <td>0.999503</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.963266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>nursery_rhymes</td>\n",
              "      <td>nursery_rhymes</td>\n",
              "      <td>0.998062</td>\n",
              "      <td>al-green</td>\n",
              "      <td>0.969548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>lin-manuel-miranda</td>\n",
              "      <td>lin-manuel-miranda</td>\n",
              "      <td>0.997963</td>\n",
              "      <td>missy-elliott</td>\n",
              "      <td>0.980488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>dr-seuss</td>\n",
              "      <td>dr-seuss</td>\n",
              "      <td>0.996994</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.983348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>blink-182</td>\n",
              "      <td>blink-182</td>\n",
              "      <td>0.999133</td>\n",
              "      <td>nursery_rhymes</td>\n",
              "      <td>0.984386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>nickelback</td>\n",
              "      <td>adele</td>\n",
              "      <td>0.999245</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.978642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>joni-mitchell</td>\n",
              "      <td>joni-mitchell</td>\n",
              "      <td>0.998963</td>\n",
              "      <td>al-green</td>\n",
              "      <td>0.972537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>adele</td>\n",
              "      <td>adele</td>\n",
              "      <td>0.999713</td>\n",
              "      <td>nursery_rhymes</td>\n",
              "      <td>0.977219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>al-green</td>\n",
              "      <td>al-green</td>\n",
              "      <td>0.999147</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.965864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>patti-smith</td>\n",
              "      <td>patti-smith</td>\n",
              "      <td>0.997954</td>\n",
              "      <td>janisjoplin</td>\n",
              "      <td>0.974032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>johnny-cash</td>\n",
              "      <td>johnny-cash</td>\n",
              "      <td>0.997940</td>\n",
              "      <td>al-green</td>\n",
              "      <td>0.971801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>dickinson</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.999454</td>\n",
              "      <td>janisjoplin</td>\n",
              "      <td>0.964522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>paul-simon</td>\n",
              "      <td>paul-simon</td>\n",
              "      <td>0.997961</td>\n",
              "      <td>al-green</td>\n",
              "      <td>0.974010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>bieber</td>\n",
              "      <td>bieber</td>\n",
              "      <td>0.999678</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.963385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>r-kelly</td>\n",
              "      <td>r-kelly</td>\n",
              "      <td>0.999281</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.972604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>lady-gaga</td>\n",
              "      <td>britney-spears</td>\n",
              "      <td>0.998924</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.963517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>bob-marley</td>\n",
              "      <td>prince</td>\n",
              "      <td>0.997906</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.977738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>michael-jackson</td>\n",
              "      <td>rihanna</td>\n",
              "      <td>0.999071</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.971215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>bjork</td>\n",
              "      <td>beatles</td>\n",
              "      <td>0.996841</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.972461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>prince</td>\n",
              "      <td>prince</td>\n",
              "      <td>0.999275</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.978409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>lil-wayne</td>\n",
              "      <td>lil-wayne</td>\n",
              "      <td>0.999680</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.967731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>cake</td>\n",
              "      <td>radiohead</td>\n",
              "      <td>0.999056</td>\n",
              "      <td>nursery_rhymes</td>\n",
              "      <td>0.982955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>disney</td>\n",
              "      <td>disney</td>\n",
              "      <td>0.999133</td>\n",
              "      <td>nursery_rhymes</td>\n",
              "      <td>0.982466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>leonard-cohen</td>\n",
              "      <td>bob-dylan</td>\n",
              "      <td>0.999297</td>\n",
              "      <td>janisjoplin</td>\n",
              "      <td>0.984172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>janisjoplin</td>\n",
              "      <td>janisjoplin</td>\n",
              "      <td>0.997625</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.954905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>bruce-springsteen</td>\n",
              "      <td>bruce-springsteen</td>\n",
              "      <td>0.999114</td>\n",
              "      <td>al-green</td>\n",
              "      <td>0.986835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>beatles</td>\n",
              "      <td>r-kelly</td>\n",
              "      <td>0.998966</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.977901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>notorious-big</td>\n",
              "      <td>notorious-big</td>\n",
              "      <td>0.999661</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.968636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>drake</td>\n",
              "      <td>kanye-west</td>\n",
              "      <td>0.999515</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.978693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>nicki-minaj</td>\n",
              "      <td>nicki-minaj</td>\n",
              "      <td>0.999474</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.969096</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                artist        most_similar      most   least_similar     least\n",
              "0              nirvana              eminem  0.998748  nursery_rhymes  0.982941\n",
              "1            bob-dylan           bob-dylan  0.999597  nursery_rhymes  0.988499\n",
              "2         dolly-parton        jimi-hendrix  0.999089       dickinson  0.981122\n",
              "3         jimi-hendrix             r-kelly  0.999065       dickinson  0.976016\n",
              "4          alicia-keys              bieber  0.999369       dickinson  0.966633\n",
              "5           kanye-west          kanye-west  0.999645       dickinson  0.974730\n",
              "6              rihanna     michael-jackson  0.999473       dickinson  0.970357\n",
              "7            dj-khaled           dj-khaled  0.999566       dickinson  0.975715\n",
              "8               eminem              eminem  0.999473       dickinson  0.981217\n",
              "9        amy-winehouse       amy-winehouse  0.999179       dickinson  0.979417\n",
              "10            ludacris            ludacris  0.999674       dickinson  0.975157\n",
              "11      britney-spears      britney-spears  0.999612       dickinson  0.965700\n",
              "12               lorde               lorde  0.998750       dickinson  0.979298\n",
              "13           radiohead                cake  0.998910  nursery_rhymes  0.985800\n",
              "14          bruno-mars          bruno-mars  0.997774       dickinson  0.957522\n",
              "15       missy-elliott       missy-elliott  0.999503       dickinson  0.963266\n",
              "16      nursery_rhymes      nursery_rhymes  0.998062        al-green  0.969548\n",
              "17  lin-manuel-miranda  lin-manuel-miranda  0.997963   missy-elliott  0.980488\n",
              "18            dr-seuss            dr-seuss  0.996994       dickinson  0.983348\n",
              "19           blink-182           blink-182  0.999133  nursery_rhymes  0.984386\n",
              "20          nickelback               adele  0.999245       dickinson  0.978642\n",
              "21       joni-mitchell       joni-mitchell  0.998963        al-green  0.972537\n",
              "22               adele               adele  0.999713  nursery_rhymes  0.977219\n",
              "23            al-green            al-green  0.999147       dickinson  0.965864\n",
              "24         patti-smith         patti-smith  0.997954     janisjoplin  0.974032\n",
              "25         johnny-cash         johnny-cash  0.997940        al-green  0.971801\n",
              "26           dickinson           dickinson  0.999454     janisjoplin  0.964522\n",
              "27          paul-simon          paul-simon  0.997961        al-green  0.974010\n",
              "28              bieber              bieber  0.999678       dickinson  0.963385\n",
              "29             r-kelly             r-kelly  0.999281       dickinson  0.972604\n",
              "30           lady-gaga      britney-spears  0.998924       dickinson  0.963517\n",
              "31          bob-marley              prince  0.997906       dickinson  0.977738\n",
              "32     michael-jackson             rihanna  0.999071       dickinson  0.971215\n",
              "33               bjork             beatles  0.996841       dickinson  0.972461\n",
              "34              prince              prince  0.999275       dickinson  0.978409\n",
              "35           lil-wayne           lil-wayne  0.999680       dickinson  0.967731\n",
              "36                cake           radiohead  0.999056  nursery_rhymes  0.982955\n",
              "37              disney              disney  0.999133  nursery_rhymes  0.982466\n",
              "38       leonard-cohen           bob-dylan  0.999297     janisjoplin  0.984172\n",
              "39         janisjoplin         janisjoplin  0.997625       dickinson  0.954905\n",
              "40   bruce-springsteen   bruce-springsteen  0.999114        al-green  0.986835\n",
              "41             beatles             r-kelly  0.998966       dickinson  0.977901\n",
              "42       notorious-big       notorious-big  0.999661       dickinson  0.968636\n",
              "43               drake          kanye-west  0.999515       dickinson  0.978693\n",
              "44         nicki-minaj         nicki-minaj  0.999474       dickinson  0.969096"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1BaN8azTaHO",
        "colab_type": "text"
      },
      "source": [
        "# Doc2Vec\n",
        "![](https://pbs.twimg.com/media/DdKR0ZOX4AEl_0N.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaryT4rkIHsL",
        "colab_type": "text"
      },
      "source": [
        "## Paragraph Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI4E3iBwIOQF",
        "colab_type": "text"
      },
      "source": [
        "### Distributed Memory\n",
        "* PV-DM\n",
        "* Analogous to Word2Vec CBOW\n",
        "* The doc-vectors are obtained by training a neural network on the synthetic task of predicting a center word based an average of both context word-vectors and the full document’s doc-vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qoj9OeYzITWB",
        "colab_type": "text"
      },
      "source": [
        "### Distributed Bag of Words \n",
        "* PV-DBOW\n",
        "* PV-DBOW is analogous to Word2Vec SkipGram. \n",
        "* The doc-vectors are obtained by training a neural network on the synthetic task of predicting a target word just from the full document’s doc-vector. (It is also common to combine this with skip-gram testing, using both the doc-vector and nearby word-vectors to predict a single target word, but only one at a time.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPO-RdntkyqT",
        "colab_type": "text"
      },
      "source": [
        "![](https://i.stack.imgur.com/t7slV.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcnLBUfMH2aL",
        "colab_type": "text"
      },
      "source": [
        "## Gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fezJ9G7ITcrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfczr9DHYwjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.build_vocab(train_corpus)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_5I4uJCYwhb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8_a8nIBbEq2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "4603f2a6-903d-4448-e4a3-7869b0e0710f"
      },
      "source": [
        "vector = model.infer_vector(['what', 'age', 'are', 'you'])\n",
        "print(vector)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.07568356 -0.15049233  0.11287741  0.24852103  0.0969092   0.06063254\n",
            " -0.02235721  0.13259408 -0.0823933   0.24891229  0.20082925  0.1533984\n",
            "  0.0744637  -0.290309    0.16146968  0.2336377  -0.433001   -0.22919615\n",
            " -0.01352799  0.13553731 -0.0786716  -0.18435977 -0.01631516 -0.00120235\n",
            "  0.30176806 -0.18565844  0.06484322  0.07710267  0.11713608  0.11530985\n",
            " -0.00547929  0.11736127  0.35931215 -0.03689464 -0.4049918  -0.01059677\n",
            "  0.1968253  -0.14210048 -0.19440651 -0.11083711  0.20551889  0.26681456\n",
            " -0.06362668  0.11165347  0.02112164  0.09843384 -0.15129791 -0.17440589\n",
            " -0.1104328   0.10460387]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BylBnTFRbROx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "47fc3dde-b530-4745-bf2c-c97fc3380c8f"
      },
      "source": [
        "ranks = []\n",
        "second_ranks = []\n",
        "for doc_id in range(len(train_corpus)):\n",
        "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
        "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
        "    rank = [docid for docid, sim in sims].index(doc_id)\n",
        "    ranks.append(rank)\n",
        "\n",
        "    second_ranks.append(sims[1])"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTqLjOG6c5SP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "17a4c932-cbed-4b79-ca4d-46b9b186ae2c"
      },
      "source": [
        "counter = collections.Counter(ranks)\n",
        "print(counter)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({0: 45})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4PxEWM-r2bk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1f44437b-cc42-4682-b89e-7a2ee34d2e85"
      },
      "source": [
        "results = []\n",
        "for doc_id in range(len(test_corpus)):\n",
        "  inferred_vector = model.infer_vector(test_corpus[doc_id].words)\n",
        "  sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
        "  max = sims[0][0]\n",
        "\n",
        "  results.append({\n",
        "      'artist' : id2artist[doc_id],\n",
        "      'most_similar' : id2artist[sims[0][0]],\n",
        "      'most': sims[0][1],\n",
        "      'least_similar' : id2artist[sims[len(sims) - 1][0]],\n",
        "      'least': sims[len(sims) - 1][1]\n",
        "  })\n",
        "pd.DataFrame(results)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>artist</th>\n",
              "      <th>most_similar</th>\n",
              "      <th>most</th>\n",
              "      <th>least_similar</th>\n",
              "      <th>least</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>nirvana</td>\n",
              "      <td>nirvana</td>\n",
              "      <td>0.747425</td>\n",
              "      <td>dj-khaled</td>\n",
              "      <td>-0.254419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bob-dylan</td>\n",
              "      <td>bob-dylan</td>\n",
              "      <td>0.730770</td>\n",
              "      <td>lil-wayne</td>\n",
              "      <td>-0.139802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dolly-parton</td>\n",
              "      <td>dolly-parton</td>\n",
              "      <td>0.819038</td>\n",
              "      <td>nicki-minaj</td>\n",
              "      <td>-0.175378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>jimi-hendrix</td>\n",
              "      <td>jimi-hendrix</td>\n",
              "      <td>0.869471</td>\n",
              "      <td>notorious-big</td>\n",
              "      <td>-0.110712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>alicia-keys</td>\n",
              "      <td>bieber</td>\n",
              "      <td>0.691594</td>\n",
              "      <td>notorious-big</td>\n",
              "      <td>0.005449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>kanye-west</td>\n",
              "      <td>kanye-west</td>\n",
              "      <td>0.939242</td>\n",
              "      <td>beatles</td>\n",
              "      <td>-0.234476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>rihanna</td>\n",
              "      <td>rihanna</td>\n",
              "      <td>0.821656</td>\n",
              "      <td>johnny-cash</td>\n",
              "      <td>-0.212295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>dj-khaled</td>\n",
              "      <td>dj-khaled</td>\n",
              "      <td>0.823635</td>\n",
              "      <td>patti-smith</td>\n",
              "      <td>-0.156614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>eminem</td>\n",
              "      <td>eminem</td>\n",
              "      <td>0.786102</td>\n",
              "      <td>joni-mitchell</td>\n",
              "      <td>-0.149733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>amy-winehouse</td>\n",
              "      <td>amy-winehouse</td>\n",
              "      <td>0.871744</td>\n",
              "      <td>lil-wayne</td>\n",
              "      <td>-0.078857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>ludacris</td>\n",
              "      <td>ludacris</td>\n",
              "      <td>0.836814</td>\n",
              "      <td>jimi-hendrix</td>\n",
              "      <td>-0.149609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>britney-spears</td>\n",
              "      <td>bieber</td>\n",
              "      <td>0.612899</td>\n",
              "      <td>johnny-cash</td>\n",
              "      <td>-0.031631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>lorde</td>\n",
              "      <td>lorde</td>\n",
              "      <td>0.786762</td>\n",
              "      <td>notorious-big</td>\n",
              "      <td>-0.089940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>radiohead</td>\n",
              "      <td>radiohead</td>\n",
              "      <td>0.840075</td>\n",
              "      <td>lil-wayne</td>\n",
              "      <td>-0.109297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>bruno-mars</td>\n",
              "      <td>bruno-mars</td>\n",
              "      <td>0.749262</td>\n",
              "      <td>johnny-cash</td>\n",
              "      <td>-0.164424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>missy-elliott</td>\n",
              "      <td>missy-elliott</td>\n",
              "      <td>0.885524</td>\n",
              "      <td>bruce-springsteen</td>\n",
              "      <td>-0.185514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>nursery_rhymes</td>\n",
              "      <td>nursery_rhymes</td>\n",
              "      <td>0.957596</td>\n",
              "      <td>drake</td>\n",
              "      <td>-0.145042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>lin-manuel-miranda</td>\n",
              "      <td>lin-manuel-miranda</td>\n",
              "      <td>0.938232</td>\n",
              "      <td>ludacris</td>\n",
              "      <td>-0.108610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>dr-seuss</td>\n",
              "      <td>dr-seuss</td>\n",
              "      <td>0.868141</td>\n",
              "      <td>adele</td>\n",
              "      <td>-0.005357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>blink-182</td>\n",
              "      <td>blink-182</td>\n",
              "      <td>0.847568</td>\n",
              "      <td>missy-elliott</td>\n",
              "      <td>-0.068286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>nickelback</td>\n",
              "      <td>nickelback</td>\n",
              "      <td>0.827331</td>\n",
              "      <td>notorious-big</td>\n",
              "      <td>-0.093804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>joni-mitchell</td>\n",
              "      <td>joni-mitchell</td>\n",
              "      <td>0.820696</td>\n",
              "      <td>lil-wayne</td>\n",
              "      <td>-0.273861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>adele</td>\n",
              "      <td>adele</td>\n",
              "      <td>0.648462</td>\n",
              "      <td>kanye-west</td>\n",
              "      <td>-0.070968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>al-green</td>\n",
              "      <td>al-green</td>\n",
              "      <td>0.783716</td>\n",
              "      <td>nicki-minaj</td>\n",
              "      <td>-0.161974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>patti-smith</td>\n",
              "      <td>patti-smith</td>\n",
              "      <td>0.675066</td>\n",
              "      <td>ludacris</td>\n",
              "      <td>-0.285013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>johnny-cash</td>\n",
              "      <td>johnny-cash</td>\n",
              "      <td>0.779793</td>\n",
              "      <td>rihanna</td>\n",
              "      <td>-0.167691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>dickinson</td>\n",
              "      <td>dickinson</td>\n",
              "      <td>0.971937</td>\n",
              "      <td>dj-khaled</td>\n",
              "      <td>-0.171743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>paul-simon</td>\n",
              "      <td>paul-simon</td>\n",
              "      <td>0.753238</td>\n",
              "      <td>drake</td>\n",
              "      <td>-0.150514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>bieber</td>\n",
              "      <td>bieber</td>\n",
              "      <td>0.900644</td>\n",
              "      <td>johnny-cash</td>\n",
              "      <td>-0.204616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>r-kelly</td>\n",
              "      <td>r-kelly</td>\n",
              "      <td>0.692377</td>\n",
              "      <td>nursery_rhymes</td>\n",
              "      <td>-0.099532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>lady-gaga</td>\n",
              "      <td>rihanna</td>\n",
              "      <td>0.655837</td>\n",
              "      <td>johnny-cash</td>\n",
              "      <td>-0.109857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>bob-marley</td>\n",
              "      <td>bob-marley</td>\n",
              "      <td>0.857910</td>\n",
              "      <td>lady-gaga</td>\n",
              "      <td>-0.132166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>michael-jackson</td>\n",
              "      <td>michael-jackson</td>\n",
              "      <td>0.640975</td>\n",
              "      <td>lil-wayne</td>\n",
              "      <td>-0.091993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>bjork</td>\n",
              "      <td>bjork</td>\n",
              "      <td>0.737972</td>\n",
              "      <td>missy-elliott</td>\n",
              "      <td>-0.153065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>prince</td>\n",
              "      <td>missy-elliott</td>\n",
              "      <td>0.643149</td>\n",
              "      <td>joni-mitchell</td>\n",
              "      <td>0.038058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>lil-wayne</td>\n",
              "      <td>lil-wayne</td>\n",
              "      <td>0.921407</td>\n",
              "      <td>joni-mitchell</td>\n",
              "      <td>-0.279393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>cake</td>\n",
              "      <td>cake</td>\n",
              "      <td>0.777452</td>\n",
              "      <td>notorious-big</td>\n",
              "      <td>-0.264063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>disney</td>\n",
              "      <td>disney</td>\n",
              "      <td>0.793122</td>\n",
              "      <td>lil-wayne</td>\n",
              "      <td>-0.246431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>leonard-cohen</td>\n",
              "      <td>leonard-cohen</td>\n",
              "      <td>0.881737</td>\n",
              "      <td>lil-wayne</td>\n",
              "      <td>-0.249632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>janisjoplin</td>\n",
              "      <td>janisjoplin</td>\n",
              "      <td>0.816728</td>\n",
              "      <td>bjork</td>\n",
              "      <td>-0.173180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>bruce-springsteen</td>\n",
              "      <td>bruce-springsteen</td>\n",
              "      <td>0.851234</td>\n",
              "      <td>notorious-big</td>\n",
              "      <td>-0.185757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>beatles</td>\n",
              "      <td>beatles</td>\n",
              "      <td>0.723645</td>\n",
              "      <td>kanye-west</td>\n",
              "      <td>-0.236222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>notorious-big</td>\n",
              "      <td>notorious-big</td>\n",
              "      <td>0.960171</td>\n",
              "      <td>beatles</td>\n",
              "      <td>-0.257986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>drake</td>\n",
              "      <td>drake</td>\n",
              "      <td>0.805298</td>\n",
              "      <td>bob-dylan</td>\n",
              "      <td>-0.167899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>nicki-minaj</td>\n",
              "      <td>nicki-minaj</td>\n",
              "      <td>0.795825</td>\n",
              "      <td>bob-dylan</td>\n",
              "      <td>-0.226800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                artist        most_similar  ...      least_similar     least\n",
              "0              nirvana             nirvana  ...          dj-khaled -0.254419\n",
              "1            bob-dylan           bob-dylan  ...          lil-wayne -0.139802\n",
              "2         dolly-parton        dolly-parton  ...        nicki-minaj -0.175378\n",
              "3         jimi-hendrix        jimi-hendrix  ...      notorious-big -0.110712\n",
              "4          alicia-keys              bieber  ...      notorious-big  0.005449\n",
              "5           kanye-west          kanye-west  ...            beatles -0.234476\n",
              "6              rihanna             rihanna  ...        johnny-cash -0.212295\n",
              "7            dj-khaled           dj-khaled  ...        patti-smith -0.156614\n",
              "8               eminem              eminem  ...      joni-mitchell -0.149733\n",
              "9        amy-winehouse       amy-winehouse  ...          lil-wayne -0.078857\n",
              "10            ludacris            ludacris  ...       jimi-hendrix -0.149609\n",
              "11      britney-spears              bieber  ...        johnny-cash -0.031631\n",
              "12               lorde               lorde  ...      notorious-big -0.089940\n",
              "13           radiohead           radiohead  ...          lil-wayne -0.109297\n",
              "14          bruno-mars          bruno-mars  ...        johnny-cash -0.164424\n",
              "15       missy-elliott       missy-elliott  ...  bruce-springsteen -0.185514\n",
              "16      nursery_rhymes      nursery_rhymes  ...              drake -0.145042\n",
              "17  lin-manuel-miranda  lin-manuel-miranda  ...           ludacris -0.108610\n",
              "18            dr-seuss            dr-seuss  ...              adele -0.005357\n",
              "19           blink-182           blink-182  ...      missy-elliott -0.068286\n",
              "20          nickelback          nickelback  ...      notorious-big -0.093804\n",
              "21       joni-mitchell       joni-mitchell  ...          lil-wayne -0.273861\n",
              "22               adele               adele  ...         kanye-west -0.070968\n",
              "23            al-green            al-green  ...        nicki-minaj -0.161974\n",
              "24         patti-smith         patti-smith  ...           ludacris -0.285013\n",
              "25         johnny-cash         johnny-cash  ...            rihanna -0.167691\n",
              "26           dickinson           dickinson  ...          dj-khaled -0.171743\n",
              "27          paul-simon          paul-simon  ...              drake -0.150514\n",
              "28              bieber              bieber  ...        johnny-cash -0.204616\n",
              "29             r-kelly             r-kelly  ...     nursery_rhymes -0.099532\n",
              "30           lady-gaga             rihanna  ...        johnny-cash -0.109857\n",
              "31          bob-marley          bob-marley  ...          lady-gaga -0.132166\n",
              "32     michael-jackson     michael-jackson  ...          lil-wayne -0.091993\n",
              "33               bjork               bjork  ...      missy-elliott -0.153065\n",
              "34              prince       missy-elliott  ...      joni-mitchell  0.038058\n",
              "35           lil-wayne           lil-wayne  ...      joni-mitchell -0.279393\n",
              "36                cake                cake  ...      notorious-big -0.264063\n",
              "37              disney              disney  ...          lil-wayne -0.246431\n",
              "38       leonard-cohen       leonard-cohen  ...          lil-wayne -0.249632\n",
              "39         janisjoplin         janisjoplin  ...              bjork -0.173180\n",
              "40   bruce-springsteen   bruce-springsteen  ...      notorious-big -0.185757\n",
              "41             beatles             beatles  ...         kanye-west -0.236222\n",
              "42       notorious-big       notorious-big  ...            beatles -0.257986\n",
              "43               drake               drake  ...          bob-dylan -0.167899\n",
              "44         nicki-minaj         nicki-minaj  ...          bob-dylan -0.226800\n",
              "\n",
              "[45 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFWg-lUftGka",
        "colab_type": "text"
      },
      "source": [
        "# BERT\n",
        "* Another approach involves use a transformer architecture\n",
        "* With a model like BERT we're:\n",
        "  * Classifying a sequence\n",
        "  * Reusing language understanding from the trained language model\n",
        "  * Fine tuning for our particular purpose\n",
        "* BERT has size limitations\n",
        "  * 512 Token limit (less [CLS] and [SEP])\n",
        "  * Truncate the documents to the max length \n",
        "  * Chunk the text (make multiple documents)\n",
        "  * Drop middle tokens (keep beginning and end)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPPdbN6bw9Gv",
        "colab_type": "text"
      },
      "source": [
        "## Pre-Training & Fine Tuning\n",
        "![](https://opendatascience.com/wp-content/uploads/2020/04/nlp7.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GabBG_AyyvX",
        "colab_type": "text"
      },
      "source": [
        "## Input\n",
        "![](https://miro.medium.com/max/2348/0*m_kXt3uqZH9e7H4w.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eay1-irfUQgk",
        "colab_type": "text"
      },
      "source": [
        "## Device\n",
        "* Test if CUDA is available, if so use the GPU else CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bojl4RenUR07",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ca1971c-bae1-4336-d5c3-5f27ac753c1f"
      },
      "source": [
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('Using GPU', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GPU Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMQVST7xo_jz",
        "colab_type": "text"
      },
      "source": [
        "## Load Model\n",
        "* Load the BERT tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs4i1JTkkRgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WVQb3mCpJph",
        "colab_type": "text"
      },
      "source": [
        "## Batch Lyrics\n",
        "* Batch up the lyrics into bundles of \"BATCH\" size\n",
        "* We create a document / training instance per batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISFmn1av9wiJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d9d1f94e-4066-4df4-fc77-c636de4fe2cd"
      },
      "source": [
        "BATCH = 200\n",
        "documents = []\n",
        "\n",
        "for words, artist_id in corpus:\n",
        "  for start in range(0, len(words), BATCH):\n",
        "    doc = words[start: start + BATCH]\n",
        "    documents.append((doc, artist_id))\n",
        "\n",
        "print(len(documents))"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5642\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38H6tLg7__FW",
        "colab_type": "text"
      },
      "source": [
        "## Encode\n",
        "* Call the tokenizer on each document\n",
        "* This will return a list of dictionaries with three items in each\n",
        "  * The encoded words\n",
        "  * The mask of the words (present or padding)\n",
        "  * The truth vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5VKO-CZACYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for doc, artist_id in documents:\n",
        "  encoded = tokenizer.encode_plus(doc, \n",
        "                                  add_special_tokens = True,\n",
        "                                  pad_to_max_length=True,\n",
        "                                  truncation = True,\n",
        "                                  return_attention_mask=True,\n",
        "                                  max_length=128)\n",
        "  X.append(encoded)\n",
        "  y.append(artist_id)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiPuOHRDBBd6",
        "colab_type": "text"
      },
      "source": [
        "## Train - Test\n",
        "* Split the encoded documents into train, test, validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LBfSZmABFaJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train)"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXxFJwa_CfSD",
        "colab_type": "text"
      },
      "source": [
        "## PyTorch Tensors\n",
        "* Convert the lists in X_train, X_test & X_val into Tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhLTEyLECo9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_inputs = torch.tensor([doc['input_ids'] for doc in X_train])\n",
        "X_test_inputs  = torch.tensor([doc['input_ids'] for doc in X_test])\n",
        "X_val_inputs   = torch.tensor([doc['input_ids'] for doc in X_val])\n",
        "\n",
        "y_train_labels = torch.tensor(y_train)\n",
        "y_test_labels  = torch.tensor(y_test)\n",
        "y_val_labels   = torch.tensor(y_val)\n",
        "\n",
        "X_train_masks  = torch.tensor([doc['attention_mask'] for doc in X_train])\n",
        "X_test_masks   = torch.tensor([doc['attention_mask'] for doc in X_test])\n",
        "X_val_masks    = torch.tensor([doc['attention_mask'] for doc in X_val])"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoMvepG_Dmqc",
        "colab_type": "text"
      },
      "source": [
        "## DataSet / Data Loader\n",
        "* Wrap up our tensors in a Dataset\n",
        "* Apply a sampler \n",
        "  * Random for Train\n",
        "  * Sequential for Test\n",
        "* Return a DataLoader that pulls in batches "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MJ3PtgkDquC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "train_data       = TensorDataset(X_train_inputs, X_train_masks, y_train_labels)\n",
        "train_sampler    = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "test_data        = TensorDataset(X_test_inputs, X_test_masks, y_test_labels)\n",
        "test_sampler     = SequentialSampler(test_data)\n",
        "test_dataloader  = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "val_data         = TensorDataset(X_val_inputs, X_val_masks, y_val_labels)\n",
        "val_sampler      = SequentialSampler(val_data)\n",
        "val_dataloader   = DataLoader(val_data, sampler=val_sampler, batch_size=BATCH_SIZE) "
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia9MPr-Mpz-j",
        "colab_type": "text"
      },
      "source": [
        "## Model\n",
        "* Use the Bert-uncased model\n",
        "* We don't need the output of attention or hidden_state (i.e. no Decode)\n",
        "* Move the model onto the GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX1LOoSJpyrr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6dde8720-2e2e-40bb-8f24-d64e959ca438"
      },
      "source": [
        "labels = len(id2artist)\n",
        "model = BertForSequenceClassification.from_pretrained (\n",
        "    \"bert-base-uncased\", # 12-layer BERT model & uncased vocabulary\n",
        "    num_labels = labels, # The number of output labels\n",
        "    output_attentions = False,    # Don't return attentions weights.\n",
        "    output_hidden_states = False, # Don't return all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=45, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG_sD9bTqJMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "    lr  = 5e-5, # args.learning_rate - default is 5e-5.\n",
        "    eps = 1e-8  # args.adam_epsilon  - default is 1e-8.\n",
        ")"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WbVQuYZqkxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * EPOCHS\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOvvEikUqqmn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfNSubFJqxnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def format_time(elapsed):\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brvsA_sjq6ix",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Xxd3yUutAxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 1003\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzK9DmTmq5_4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dd265f9c-c777-4e96-888c-852897264025"
      },
      "source": [
        "# Average Epoch Losses\n",
        "loss_values = []\n",
        "\n",
        "# Training Loop\n",
        "for epoch_i in range(0, EPOCHS):\n",
        "  print(\"\")\n",
        "  print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, EPOCHS))\n",
        "  print('Training...')\n",
        "\n",
        "  # Measure how long the training epoch takes.\n",
        "  t0 = time.time()\n",
        "\n",
        "  # Reset the total loss for this epoch.\n",
        "  total_loss = 0\n",
        "\n",
        "  # Training mode\n",
        "  model.train()\n",
        "\n",
        "  # For each batch of training data...\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "    # Progress update every 100 batches.\n",
        "    if step % 20 == 0 and not step == 0:\n",
        "        # Calculate elapsed time in minutes.\n",
        "        elapsed = format_time(time.time() - t0)\n",
        "        \n",
        "        # Report progress.\n",
        "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "    # Use the dataloader to grab a batch\n",
        "    # Unpack into \"b_\" batch variables\n",
        "    #   input ids \n",
        "    #   attention masks\n",
        "    #   labels \n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "\n",
        "    # Clear previous gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # Forward pass \n",
        "    outputs = model(b_input_ids, \n",
        "                token_type_ids=None, \n",
        "                attention_mask=b_input_mask, \n",
        "                labels=b_labels)\n",
        "    \n",
        "    # Take loss from the forward pass output\n",
        "    loss = outputs[0]\n",
        "\n",
        "    # Accumulate the training loss \n",
        "    total_loss += loss.item()\n",
        "\n",
        "    # Backward pass / calc gradients.\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients -- Prevent \"exploding gradients\"\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # Update parameters (i.e. weights)\n",
        "    # Step using the gradient.\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update the learning rate with our scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "  # Calculate the average loss over the training data.\n",
        "  avg_train_loss = total_loss / len(train_dataloader)            \n",
        "  \n",
        "  # Store the loss value for plotting the learning curve.\n",
        "  loss_values.append(avg_train_loss)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "  print(\"  Training Epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "      \n",
        "  # Validation\n",
        "  print(\"\")\n",
        "  print(\"Running Validation...\")\n",
        "\n",
        "  t0 = time.time()\n",
        "\n",
        "  # Evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in val_dataloader:    \n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model not to compute or store gradients, saving memory and\n",
        "    # speeding up validation\n",
        "    with torch.no_grad():        \n",
        "      outputs = model(b_input_ids, \n",
        "                      token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "    \n",
        "    # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "    # values prior to applying an activation function like the softmax.\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # Calculate the accuracy for this batch \n",
        "    # Flat_accuracy = flattened\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    \n",
        "    # Accumulate \n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    # Track the number of batches\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "  # Final accuracy\n",
        "  print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "  print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    20  of    397.    Elapsed: 0:00:05.\n",
            "  Batch    40  of    397.    Elapsed: 0:00:09.\n",
            "  Batch    60  of    397.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    397.    Elapsed: 0:00:18.\n",
            "  Batch   100  of    397.    Elapsed: 0:00:23.\n",
            "  Batch   120  of    397.    Elapsed: 0:00:28.\n",
            "  Batch   140  of    397.    Elapsed: 0:00:32.\n",
            "  Batch   160  of    397.    Elapsed: 0:00:37.\n",
            "  Batch   180  of    397.    Elapsed: 0:00:41.\n",
            "  Batch   200  of    397.    Elapsed: 0:00:46.\n",
            "  Batch   220  of    397.    Elapsed: 0:00:50.\n",
            "  Batch   240  of    397.    Elapsed: 0:00:55.\n",
            "  Batch   260  of    397.    Elapsed: 0:00:59.\n",
            "  Batch   280  of    397.    Elapsed: 0:01:04.\n",
            "  Batch   300  of    397.    Elapsed: 0:01:08.\n",
            "  Batch   320  of    397.    Elapsed: 0:01:13.\n",
            "  Batch   340  of    397.    Elapsed: 0:01:17.\n",
            "  Batch   360  of    397.    Elapsed: 0:01:22.\n",
            "  Batch   380  of    397.    Elapsed: 0:01:26.\n",
            "\n",
            "  Average training loss: 3.54\n",
            "  Training Epoch took: 0:01:30\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.14\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    20  of    397.    Elapsed: 0:00:04.\n",
            "  Batch    40  of    397.    Elapsed: 0:00:09.\n",
            "  Batch    60  of    397.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    397.    Elapsed: 0:00:18.\n",
            "  Batch   100  of    397.    Elapsed: 0:00:23.\n",
            "  Batch   120  of    397.    Elapsed: 0:00:27.\n",
            "  Batch   140  of    397.    Elapsed: 0:00:32.\n",
            "  Batch   160  of    397.    Elapsed: 0:00:36.\n",
            "  Batch   180  of    397.    Elapsed: 0:00:41.\n",
            "  Batch   200  of    397.    Elapsed: 0:00:45.\n",
            "  Batch   220  of    397.    Elapsed: 0:00:50.\n",
            "  Batch   240  of    397.    Elapsed: 0:00:54.\n",
            "  Batch   260  of    397.    Elapsed: 0:00:58.\n",
            "  Batch   280  of    397.    Elapsed: 0:01:03.\n",
            "  Batch   300  of    397.    Elapsed: 0:01:07.\n",
            "  Batch   320  of    397.    Elapsed: 0:01:12.\n",
            "  Batch   340  of    397.    Elapsed: 0:01:16.\n",
            "  Batch   360  of    397.    Elapsed: 0:01:21.\n",
            "  Batch   380  of    397.    Elapsed: 0:01:25.\n",
            "\n",
            "  Average training loss: 3.00\n",
            "  Training Epoch took: 0:01:29\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.16\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    20  of    397.    Elapsed: 0:00:05.\n",
            "  Batch    40  of    397.    Elapsed: 0:00:09.\n",
            "  Batch    60  of    397.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    397.    Elapsed: 0:00:18.\n",
            "  Batch   100  of    397.    Elapsed: 0:00:22.\n",
            "  Batch   120  of    397.    Elapsed: 0:00:27.\n",
            "  Batch   140  of    397.    Elapsed: 0:00:31.\n",
            "  Batch   160  of    397.    Elapsed: 0:00:36.\n",
            "  Batch   180  of    397.    Elapsed: 0:00:40.\n",
            "  Batch   200  of    397.    Elapsed: 0:00:45.\n",
            "  Batch   220  of    397.    Elapsed: 0:00:49.\n",
            "  Batch   240  of    397.    Elapsed: 0:00:54.\n",
            "  Batch   260  of    397.    Elapsed: 0:00:58.\n",
            "  Batch   280  of    397.    Elapsed: 0:01:03.\n",
            "  Batch   300  of    397.    Elapsed: 0:01:07.\n",
            "  Batch   320  of    397.    Elapsed: 0:01:12.\n",
            "  Batch   340  of    397.    Elapsed: 0:01:16.\n",
            "  Batch   360  of    397.    Elapsed: 0:01:21.\n",
            "  Batch   380  of    397.    Elapsed: 0:01:25.\n",
            "\n",
            "  Average training loss: 2.66\n",
            "  Training Epoch took: 0:01:29\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.22\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    20  of    397.    Elapsed: 0:00:04.\n",
            "  Batch    40  of    397.    Elapsed: 0:00:09.\n",
            "  Batch    60  of    397.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    397.    Elapsed: 0:00:18.\n",
            "  Batch   100  of    397.    Elapsed: 0:00:22.\n",
            "  Batch   120  of    397.    Elapsed: 0:00:27.\n",
            "  Batch   140  of    397.    Elapsed: 0:00:31.\n",
            "  Batch   160  of    397.    Elapsed: 0:00:36.\n",
            "  Batch   180  of    397.    Elapsed: 0:00:40.\n",
            "  Batch   200  of    397.    Elapsed: 0:00:45.\n",
            "  Batch   220  of    397.    Elapsed: 0:00:49.\n",
            "  Batch   240  of    397.    Elapsed: 0:00:54.\n",
            "  Batch   260  of    397.    Elapsed: 0:00:58.\n",
            "  Batch   280  of    397.    Elapsed: 0:01:03.\n",
            "  Batch   300  of    397.    Elapsed: 0:01:07.\n",
            "  Batch   320  of    397.    Elapsed: 0:01:12.\n",
            "  Batch   340  of    397.    Elapsed: 0:01:16.\n",
            "  Batch   360  of    397.    Elapsed: 0:01:21.\n",
            "  Batch   380  of    397.    Elapsed: 0:01:25.\n",
            "\n",
            "  Average training loss: 2.34\n",
            "  Training Epoch took: 0:01:29\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.27\n",
            "  Validation took: 0:00:09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUUOf4km3Wqw",
        "colab_type": "text"
      },
      "source": [
        "## Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNhT4dOW3cXN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "8af0318d-a833-4b05-9695-39a8e37ff90d"
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(loss_values)\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss');"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPCTL4vqWNVS",
        "colab_type": "text"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlwdFy_eT5yy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "b723403a-069b-4a77-bbe6-1f0dcfb9c828"
      },
      "source": [
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Measure elapsed time.\n",
        "t0 = time.time()\n",
        "\n",
        "# Predict \n",
        "for (step, batch) in enumerate(test_dataloader):\n",
        "    \n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "    # Progress update every 100 batches.\n",
        "    if step % 20 == 0 and not step == 0:\n",
        "        # Calculate elapsed time in minutes.\n",
        "        elapsed = format_time(time.time() - t0)\n",
        "        \n",
        "        # Report progress.\n",
        "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
        "\n",
        "\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Batch    20  of    177.    Elapsed: 0:00:01.\n",
            "  Batch    40  of    177.    Elapsed: 0:00:03.\n",
            "  Batch    60  of    177.    Elapsed: 0:00:04.\n",
            "  Batch    80  of    177.    Elapsed: 0:00:06.\n",
            "  Batch   100  of    177.    Elapsed: 0:00:07.\n",
            "  Batch   120  of    177.    Elapsed: 0:00:09.\n",
            "  Batch   140  of    177.    Elapsed: 0:00:10.\n",
            "  Batch   160  of    177.    Elapsed: 0:00:12.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXWrni1YWWGr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.concatenate(predictions, axis=0)\n",
        "predictions_class = np.argmax(predictions, axis=1)\n",
        "\n",
        "true_labels_agg = np.concatenate(true_labels, axis=0)\n",
        "true_labels_class = true_labels_agg.T.squeeze()"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kj6FrrO53QVM",
        "colab_type": "text"
      },
      "source": [
        "## Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOqmbLXPWlim",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "outputId": "08ecda1e-c6ec-4433-b947-6c8d2e3010de"
      },
      "source": [
        "print(classification_report(true_labels_class, predictions_class, target_names=id2artist))"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                    precision    recall  f1-score   support\n",
            "\n",
            "           nirvana       0.00      0.00      0.00        13\n",
            "         bob-dylan       0.22      0.76      0.33        49\n",
            "      dolly-parton       0.00      0.00      0.00        23\n",
            "      jimi-hendrix       0.00      0.00      0.00        20\n",
            "       alicia-keys       0.00      0.00      0.00        34\n",
            "        kanye-west       0.39      0.79      0.52        61\n",
            "           rihanna       0.00      0.00      0.00        34\n",
            "         dj-khaled       0.30      0.17      0.22        41\n",
            "            eminem       0.34      0.73      0.47        82\n",
            "     amy-winehouse       0.00      0.00      0.00        20\n",
            "          ludacris       0.38      0.34      0.36        53\n",
            "    britney-spears       0.12      0.23      0.15        31\n",
            "             lorde       0.00      0.00      0.00        16\n",
            "         radiohead       0.00      0.00      0.00        13\n",
            "        bruno-mars       0.00      0.00      0.00        36\n",
            "     missy-elliott       0.68      0.50      0.57        50\n",
            "    nursery_rhymes       0.00      0.00      0.00        16\n",
            "lin-manuel-miranda       0.00      0.00      0.00         6\n",
            "          dr-seuss       0.00      0.00      0.00        10\n",
            "         blink-182       0.00      0.00      0.00        22\n",
            "        nickelback       0.27      0.32      0.29        41\n",
            "     joni-mitchell       0.00      0.00      0.00        23\n",
            "             adele       0.10      0.53      0.17        17\n",
            "          al-green       0.00      0.00      0.00        25\n",
            "       patti-smith       0.00      0.00      0.00        16\n",
            "       johnny-cash       0.00      0.00      0.00        19\n",
            "         dickinson       0.53      0.95      0.68        39\n",
            "        paul-simon       0.00      0.00      0.00        17\n",
            "            bieber       0.13      0.52      0.20        29\n",
            "           r-kelly       0.29      0.04      0.08        46\n",
            "         lady-gaga       0.29      0.05      0.09        38\n",
            "        bob-marley       0.00      0.00      0.00        19\n",
            "   michael-jackson       0.13      0.54      0.22        52\n",
            "             bjork       0.00      0.00      0.00        15\n",
            "            prince       0.46      0.64      0.54        69\n",
            "         lil-wayne       0.00      0.00      0.00        23\n",
            "              cake       0.00      0.00      0.00        16\n",
            "            disney       0.00      0.00      0.00        10\n",
            "     leonard-cohen       0.03      0.05      0.04        22\n",
            "       janisjoplin       0.50      0.30      0.37        27\n",
            " bruce-springsteen       0.11      0.03      0.05        32\n",
            "           beatles       0.00      0.00      0.00        20\n",
            "     notorious-big       0.30      0.12      0.17        57\n",
            "             drake       0.31      0.07      0.12        56\n",
            "       nicki-minaj       0.25      0.13      0.17        53\n",
            "\n",
            "          accuracy                           0.27      1411\n",
            "         macro avg       0.14      0.17      0.13      1411\n",
            "      weighted avg       0.21      0.27      0.20      1411\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-EH_4zD4Kqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}