{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPNamedEntityRecognition",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOd7AelF12LTPBwm452mK7S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swilsonmfc/nlp/blob/master/NLPNamedEntityRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyip83LTH1lw",
        "colab_type": "text"
      },
      "source": [
        "# Named Entity Recognition\n",
        "![alt text](https://cdn.britannica.com/55/188355-050-D5E49258/Salvatore-Corsitto-The-Godfather-Marlon-Brando-Francis.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmF_3XuEILMa",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvYfqhwjIbQZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "6a0acb62-7430-4b64-979a-e5d873a84eb8"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CatZ31rMIffz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "06ec4151-5330-4ef4-d225-cc4b6aed641b"
      },
      "source": [
        "pip install wget"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QqjyWndH9f_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "456e5c18-abb6-4dba-fa3e-332702724561"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import wget\n",
        "import csv\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import pprint as pp\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForTokenClassification\n",
        "from transformers import AdamW\n",
        "from transformers import BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import RandomSampler\n",
        "from torch.utils.data import SequentialSampler\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7BwC4tdVnJg",
        "colab_type": "text"
      },
      "source": [
        "# BERT\n",
        "* Fine-tuning BERT for token classification\n",
        "* BERT base & uncased\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/bert-base-bert-large-encoders.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTz9ORQ7VoyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2cxwuZ7IRnv",
        "colab_type": "text"
      },
      "source": [
        "# GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Go60s9KIOAS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2a3fdefb-17f7-4862-b259-2c807e8b8f97"
      },
      "source": [
        "if torch.cuda.is_available():       \n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s).')\n",
        "    print(f'GPU Type: {torch.cuda.get_device_name(0)}')\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print('No GPU available, using CPU.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s).\n",
            "GPU Type: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDw_lzG9IvBr",
        "colab_type": "text"
      },
      "source": [
        "# Data\n",
        "* Using the MIT Movie Task\n",
        "* Formatted in BIO (or IOB) Format\n",
        "* BIO = Beginning, Inside, Outside\n",
        "\n",
        "TAG | EXAMPLE\n",
        "--- | ---\n",
        "ACTOR|Matt Damon\n",
        "YEAR|1980s\n",
        "TITLE|Pulp Fiction\n",
        "GENRE|science fiction\n",
        "DIRECTOR|George Lucas\n",
        "SONG|Aerosmith\n",
        "PLOT|Flying cars\n",
        "REVIEW|must see\n",
        "CHARACTER|Queen Elizabeth\n",
        "RATING|PG-13\n",
        "RATINGS_AVERAGE|best rated\n",
        "TRAILER|preview\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6pcYXGFIvwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wget.download('https://groups.csail.mit.edu/sls/downloads/movie/engtrain.bio', './engtrain.bio')\n",
        "wget.download('https://groups.csail.mit.edu/sls/downloads/movie/engtest.bio',  './engtest.bio')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHRBs7o4JI8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku0ULMQIJgu0",
        "colab_type": "text"
      },
      "source": [
        "# Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbyUYBCyJM6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_bio(file):\n",
        "  \"\"\"\n",
        "  We read along, pushing tokens & any labels onto a local list \n",
        "  When we hit an empty line, the sentence is constructed\n",
        "  Append it to the sentences & labels lists, reset tokens and continue\n",
        "  \"\"\"\n",
        "  sentences = []\n",
        "  labels = []\n",
        "\n",
        "  # Lists to store the current iteration / sentence\n",
        "  tokens = []\n",
        "  token_labels = []\n",
        "\n",
        "  with open(file, newline = '') as lines:                                                                                          \n",
        "      reader = csv.reader(lines, delimiter='\\t')\n",
        "      for line in reader:\n",
        "          if line == []:\n",
        "              # Store full sentence\n",
        "              sentences.append(tokens)\n",
        "              labels.append(token_labels)           \n",
        "      \n",
        "              # Start new\n",
        "              tokens = []\n",
        "              token_labels = []        \n",
        "\n",
        "          else: \n",
        "              # Append to the sentence buffer\n",
        "              tokens.append(line[1])\n",
        "              token_labels.append(line[0])\n",
        "\n",
        "  return sentences, labels"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMkjaWnQqxvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences, labels = read_bio('./engtrain.bio')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13AFAlSxQuQ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "09fbd591-68a8-4538-d72d-8066c7822408"
      },
      "source": [
        "for i in range(5):\n",
        "  print(f'{\" \".join(sentences[i])}')\n",
        "  print(f'{\" \".join(labels[i])}')\n",
        "  print('\\n')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "what movies star bruce willis\n",
            "O O O B-ACTOR I-ACTOR\n",
            "\n",
            "\n",
            "show me films with drew barrymore from the 1980s\n",
            "O O O O B-ACTOR I-ACTOR O O B-YEAR\n",
            "\n",
            "\n",
            "what movies starred both al pacino and robert deniro\n",
            "O O O O B-ACTOR I-ACTOR O B-ACTOR I-ACTOR\n",
            "\n",
            "\n",
            "find me all of the movies that starred harold ramis and bill murray\n",
            "O O O O O O O O B-ACTOR I-ACTOR O B-ACTOR I-ACTOR\n",
            "\n",
            "\n",
            "find me a movie with a quote about baseball in it\n",
            "O O O O O O O O O O O\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZI_RCK6Vv-R",
        "colab_type": "text"
      },
      "source": [
        "# Tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQLpaHEHqBjI",
        "colab_type": "text"
      },
      "source": [
        "## Label to Id Index\n",
        "* Create a mapping from our BIO labels to integer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVYYiASGORZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_label_index(labels):\n",
        "  \"\"\"\n",
        "  Given a list of sentence labels (list of lists)\n",
        "  Construct a dictionary mapping each label to a unique integer\n",
        "  \"\"\"\n",
        "  unique  = set([label for sentence_labels in labels for label in sentence_labels])\n",
        "  mapping = {k: v for v, k in enumerate(unique)} \n",
        "  return mapping"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPrQCA715Z6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label2id = generate_label_index(labels)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrZ18uHS9-er",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "4eab7d1b-6b0f-44d6-b5dc-87da9b43371e"
      },
      "source": [
        "pp.pprint(label2id)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'B-ACTOR': 7,\n",
            " 'B-CHARACTER': 18,\n",
            " 'B-DIRECTOR': 0,\n",
            " 'B-GENRE': 2,\n",
            " 'B-PLOT': 5,\n",
            " 'B-RATING': 12,\n",
            " 'B-RATINGS_AVERAGE': 15,\n",
            " 'B-REVIEW': 13,\n",
            " 'B-SONG': 20,\n",
            " 'B-TITLE': 8,\n",
            " 'B-TRAILER': 3,\n",
            " 'B-YEAR': 1,\n",
            " 'I-ACTOR': 6,\n",
            " 'I-CHARACTER': 16,\n",
            " 'I-DIRECTOR': 22,\n",
            " 'I-GENRE': 19,\n",
            " 'I-PLOT': 21,\n",
            " 'I-RATING': 11,\n",
            " 'I-RATINGS_AVERAGE': 14,\n",
            " 'I-REVIEW': 4,\n",
            " 'I-SONG': 9,\n",
            " 'I-TITLE': 24,\n",
            " 'I-TRAILER': 17,\n",
            " 'I-YEAR': 23,\n",
            " 'O': 10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpcNE8RpqGhS",
        "colab_type": "text"
      },
      "source": [
        "## Apply BERT Tokens\n",
        "* Encode the data for BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZiVAVk8QQbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(sentences):\n",
        "  \"\"\"\n",
        "  Join sentence from token back into string for BERT\n",
        "  Take in the list of sentences and use the BERT encode_plus to:\n",
        "  * Add [CLS] to the start\n",
        "  * Add [SEP] to the end\n",
        "  * Add [PAD] to max length\n",
        "  * Map any tokens to an identifier\n",
        "  * Unknown tokens split using double-hash\n",
        "  * Return our attention mask and pytorch tensors\n",
        "  \"\"\"\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "\n",
        "  for sentence in sentences:\n",
        "      text = ' '.join(sentence)\n",
        "      encoded_dict = tokenizer.encode_plus(text, \n",
        "                                           add_special_tokens = True, \n",
        "                                           truncation = True,\n",
        "                                           max_length = 50,  \n",
        "                                           pad_to_max_length = True,\n",
        "                                           return_attention_mask = True,   \n",
        "                                           return_tensors = 'pt')\n",
        "      \n",
        "      # Append and process next sentence\n",
        "      input_ids.append(encoded_dict['input_ids'][0])\n",
        "      attention_masks.append(encoded_dict['attention_mask'][0])\n",
        "\n",
        "  return input_ids, attention_masks"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEutVYvX5Yju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids, attention_masks = tokenize(sentences)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqQJyU8K-KAc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e19ec946-b7cf-4ace-c613-5ae74495876e"
      },
      "source": [
        "pp.pprint(input_ids[2])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([  101,  2054,  5691,  5652,  2119,  2632, 14397,  5740,  1998,  2728,\n",
            "         7939,  9711,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be3cv3s--OQ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8ba5237b-e988-44ab-e157-9900af3075cc"
      },
      "source": [
        "pp.pprint(attention_masks[2])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stUwN-jeqLUN",
        "colab_type": "text"
      },
      "source": [
        "## Apply Custom Token\n",
        "* Interject our custom NER tokens into the BERT encodings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xepUdRQBeNOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SPECIAL_TOKENS = [tokenizer.pad_token_id, \n",
        "                  tokenizer.cls_token_id, \n",
        "                  tokenizer.sep_token_id]\n",
        "NULL_LABEL_ID  = -100\n",
        "\n",
        "def label_sentences(input_ids, labels):\n",
        "  \"\"\"\n",
        "  BERT will add tokens and break apart unknown tokens\n",
        "  We encode these with our null indicator and label the tokens with our index\n",
        "  \n",
        "  For example:\n",
        "  * We have the sentence: Robert Deniro\n",
        "  * Our tokens are      : B-Actor, I-Actor\n",
        "  * Our token map       : B-Actor : 0\n",
        "                        : I-Actor : 18\n",
        "  * BERT made it [CLS] Rob ##ert De ##ni ##ro [SEP] [PAD] ...\n",
        "  * We need to apply our BIO tokens into the BERT tokenized list\n",
        "    - Skip any special tokens BERT adds\n",
        "    - Skip any ## tokens\n",
        "    - Then apply our token\n",
        "  * Our BERT encoded value should be -100, 0, -100, 18, -100, -100, -100 ...\n",
        "  \"\"\"\n",
        "  new_labels = []\n",
        "\n",
        "  for (sentence, orig_labels) in zip(input_ids, labels):\n",
        "      padded_labels = []\n",
        "      orig_labels_index = 0 \n",
        "\n",
        "      for token_id in sentence:\n",
        "        token_id = token_id.numpy().item()\n",
        "\n",
        "        if token_id in SPECIAL_TOKENS or tokenizer.ids_to_tokens[token_id][0:2] == '##':  \n",
        "          padded_labels.append(NULL_LABEL_ID)\n",
        "        else:\n",
        "          label_str = orig_labels[orig_labels_index]\n",
        "          padded_labels.append(label2id[label_str])\n",
        "          orig_labels_index += 1\n",
        "\n",
        "      new_labels.append(padded_labels)\n",
        "\n",
        "  return new_labels"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnUZn-HD5etO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_labels = label_sentences(input_ids, labels)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyVRBXPh6yRT",
        "colab_type": "text"
      },
      "source": [
        "## Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqitEA1WhnhF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "2eaa0b66-a5cf-41cf-ea40-404bdd63d2bb"
      },
      "source": [
        "print('\\nSentence:    ', sentences[2])\n",
        "print('\\nLabels:      ', labels[2])\n",
        "print('\\nBERT Tokens: ', tokenizer.tokenize(' '.join(sentences[2])))\n",
        "print('\\nToken IDs:   ', input_ids[2])\n",
        "print('\\nNew Labels:  ', new_labels[2])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Sentence:     ['what', 'movies', 'starred', 'both', 'al', 'pacino', 'and', 'robert', 'deniro']\n",
            "\n",
            "Labels:       ['O', 'O', 'O', 'O', 'B-ACTOR', 'I-ACTOR', 'O', 'B-ACTOR', 'I-ACTOR']\n",
            "\n",
            "BERT Tokens:  ['what', 'movies', 'starred', 'both', 'al', 'pac', '##ino', 'and', 'robert', 'den', '##iro']\n",
            "\n",
            "Token IDs:    tensor([  101,  2054,  5691,  5652,  2119,  2632, 14397,  5740,  1998,  2728,\n",
            "         7939,  9711,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "\n",
            "New Labels:   [-100, 10, 10, 10, 10, 7, 6, -100, 10, 7, 6, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlAnnFnWmTJ_",
        "colab_type": "text"
      },
      "source": [
        "## Tokens to Tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MREOQysi58z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokens_to_tensors(input_ids, attention_masks, new_labels):\n",
        "  \"\"\"\n",
        "  Create pytorch tensors of correct size to train \n",
        "  \"\"\"\n",
        "  pt_input_ids       = torch.stack(input_ids, dim=0)\n",
        "  pt_attention_masks = torch.stack(attention_masks, dim=0)\n",
        "  pt_labels          = torch.tensor(new_labels, dtype=torch.long)\n",
        "  return pt_input_ids, pt_attention_masks, pt_labels\n",
        "\n",
        "pt_input_ids, pt_attention_masks, pt_labels = \\\n",
        "  tokens_to_tensors(input_ids, attention_masks, new_labels)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnbobDuGcWgb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "9c53058f-4bc8-4a45-c869-0bf26ee29176"
      },
      "source": [
        "print('PyTorch Tensor Input Embeddings')\n",
        "print(pt_input_ids)\n",
        "\n",
        "print('PyTorch Tensor Attention Masks')\n",
        "print(pt_attention_masks)\n",
        "\n",
        "print('PyTorch Tensor Labels')\n",
        "print(pt_labels)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch Tensor Input Embeddings\n",
            "tensor([[ 101, 2054, 5691,  ...,    0,    0,    0],\n",
            "        [ 101, 2265, 2033,  ...,    0,    0,    0],\n",
            "        [ 101, 2054, 5691,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 2054, 2079,  ...,    0,    0,    0],\n",
            "        [ 101, 2265, 2033,  ...,    0,    0,    0],\n",
            "        [ 101, 1045, 2215,  ...,    0,    0,    0]])\n",
            "PyTorch Tensor Attention Masks\n",
            "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])\n",
            "PyTorch Tensor Labels\n",
            "tensor([[-100,   10,   10,  ..., -100, -100, -100],\n",
            "        [-100,   10,   10,  ..., -100, -100, -100],\n",
            "        [-100,   10,   10,  ..., -100, -100, -100],\n",
            "        ...,\n",
            "        [-100,   10,   10,  ..., -100, -100, -100],\n",
            "        [-100,   10,   10,  ..., -100, -100, -100],\n",
            "        [-100,   10,   10,  ..., -100, -100, -100]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWPVA_4tUyfv",
        "colab_type": "text"
      },
      "source": [
        "# BERT & Transformers\n",
        "* Images from http://jalammar.github.io/\n",
        "* For our example, we're using a length of 50\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/bert-input-output.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAfqxg6rVZz_",
        "colab_type": "text"
      },
      "source": [
        "![alt text](http://jalammar.github.io/images/bert-encoders-input.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3RnCSSnVdJb",
        "colab_type": "text"
      },
      "source": [
        "![alt text](http://jalammar.github.io/images/t/encoder_with_tensors_2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWVKEN39WPRI",
        "colab_type": "text"
      },
      "source": [
        "## Self-Attention \n",
        "* Query\n",
        "* Key\n",
        "* Value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMe4Lw9mV92P",
        "colab_type": "text"
      },
      "source": [
        "![alt text](http://jalammar.github.io/images/t/self-attention-matrix-calculation.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opuKHDduWaPe",
        "colab_type": "text"
      },
      "source": [
        "## Output - Z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0pL6NscWU3N",
        "colab_type": "text"
      },
      "source": [
        "![alt text](http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC-qNY3GXH16",
        "colab_type": "text"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPGbep_nXAzZ",
        "colab_type": "text"
      },
      "source": [
        "![alt text](http://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp36Pt7uRnqt",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wizN4kAGmSBH",
        "colab_type": "text"
      },
      "source": [
        "## Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bme7_wDTmShU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9ca4e824-44c2-469a-cbff-903ec38004b5"
      },
      "source": [
        "dataset = TensorDataset(pt_input_ids, pt_attention_masks, pt_labels)\n",
        "\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size   = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print(f'{train_size:,} training examples')\n",
        "print(f'{val_size:,} validation examples')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8,797 training examples\n",
            "978 validation examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEyxTlZrma8v",
        "colab_type": "text"
      },
      "source": [
        "## Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAhh5Y9S78kp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm8fEaCumeeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  \n",
        "            sampler = RandomSampler(train_dataset), \n",
        "            batch_size = BATCH_SIZE)\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, \n",
        "            sampler = SequentialSampler(val_dataset),\n",
        "            batch_size = BATCH_SIZE)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA6gc-3Umsw-",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boVugC0ZmuEV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "052e4265-4497-4aa9-e1e9-7c711e084460"
      },
      "source": [
        "# Load BertForTokenClassification \n",
        "model = BertForTokenClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", \n",
        "    num_labels = len(label2id) + 1, \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False)\n",
        "\n",
        "# Run model on GPU.\n",
        "model.cuda();"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWu5-XFpm0qV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr = 5e-5, eps = 1e-8)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhVHDOivm22d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 4\n",
        "total_steps = len(train_dataloader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7RFFJCYnINC",
        "colab_type": "text"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKMI1n9onMSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    return str(datetime.timedelta(seconds=int(round(elapsed))))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUtTTlVinJSr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "aac1b534-1529-40fd-8b3a-f7c8f36583ea"
      },
      "source": [
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "# Includes refinements from Chris McCormick AI https://www.chrismccormick.ai/\n",
        "seed_val = 1337\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, EPOCHS):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, EPOCHS))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    275.    Elapsed: 0:00:12.\n",
            "  Batch    80  of    275.    Elapsed: 0:00:24.\n",
            "  Batch   120  of    275.    Elapsed: 0:00:36.\n",
            "  Batch   160  of    275.    Elapsed: 0:00:48.\n",
            "  Batch   200  of    275.    Elapsed: 0:01:00.\n",
            "  Batch   240  of    275.    Elapsed: 0:01:12.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epcoh took: 0:01:22\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    275.    Elapsed: 0:00:12.\n",
            "  Batch    80  of    275.    Elapsed: 0:00:24.\n",
            "  Batch   120  of    275.    Elapsed: 0:00:36.\n",
            "  Batch   160  of    275.    Elapsed: 0:00:47.\n",
            "  Batch   200  of    275.    Elapsed: 0:00:59.\n",
            "  Batch   240  of    275.    Elapsed: 0:01:11.\n",
            "\n",
            "  Average training loss: 0.17\n",
            "  Training epcoh took: 0:01:21\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    275.    Elapsed: 0:00:12.\n",
            "  Batch    80  of    275.    Elapsed: 0:00:24.\n",
            "  Batch   120  of    275.    Elapsed: 0:00:35.\n",
            "  Batch   160  of    275.    Elapsed: 0:00:47.\n",
            "  Batch   200  of    275.    Elapsed: 0:00:59.\n",
            "  Batch   240  of    275.    Elapsed: 0:01:11.\n",
            "\n",
            "  Average training loss: 0.12\n",
            "  Training epcoh took: 0:01:21\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    275.    Elapsed: 0:00:12.\n",
            "  Batch    80  of    275.    Elapsed: 0:00:24.\n",
            "  Batch   120  of    275.    Elapsed: 0:00:35.\n",
            "  Batch   160  of    275.    Elapsed: 0:00:47.\n",
            "  Batch   200  of    275.    Elapsed: 0:00:59.\n",
            "  Batch   240  of    275.    Elapsed: 0:01:11.\n",
            "\n",
            "  Average training loss: 0.09\n",
            "  Training epcoh took: 0:01:21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GidKWI2HRx3C",
        "colab_type": "text"
      },
      "source": [
        "## Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4o65e2VGkxq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "cedbd1db-4d49-4cd5-baf7-626985afe522"
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(loss_values)\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epochs')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Epochs')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jaL2K-Pppmw",
        "colab_type": "text"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak-ROBbeqtaZ",
        "colab_type": "text"
      },
      "source": [
        "## Load Test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsqXyyOapyFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sentences, test_labels = read_bio('./engtest.bio')\n",
        "test_input_ids, test_attention_masks = tokenize(test_sentences)\n",
        "test_new_labels = label_sentences(test_input_ids, test_labels)\n",
        "test_pt_input_ids, test_pt_attention_masks, test_pt_labels = \\\n",
        "  tokens_to_tensors(test_input_ids, test_attention_masks, test_new_labels)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-SWvgBFrJNB",
        "colab_type": "text"
      },
      "source": [
        "## Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHhx4qcqsY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction_data = TensorDataset(test_pt_input_ids, test_pt_attention_masks, test_pt_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Bx7ZSKyrWiy",
        "colab_type": "text"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QXE_DZVrYTA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f524f5a-6f6d-46f3-cfb7-bb59807473f1"
      },
      "source": [
        "print(f'Predicting labels for {len(test_pt_input_ids):,} test set')\n",
        "\n",
        "model.eval()\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  \n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack from dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Don't compute gradients\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logits\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move predictions & labels to cpu\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 2,443 test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zx0zhFjrtCS",
        "colab_type": "text"
      },
      "source": [
        "## Convert\n",
        "* We get a softmax of predictions per label by label type (for each sentence)\n",
        "* We take the largest probability as the label\n",
        "* Then flatten the sentences which gives us prediction per token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_FBVqmJrmIt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0b1834f8-7968-430e-9984-23376345e332"
      },
      "source": [
        "# First, combine the results across the batches.\n",
        "all_predictions = np.concatenate(predictions, axis=0)\n",
        "all_true_labels = np.concatenate(true_labels, axis=0)\n",
        "print(f'Predictions shape: {all_predictions.shape}')\n",
        "\n",
        "# Pick argmax from predicted label scores\n",
        "predicted_label_ids = np.argmax(all_predictions, axis=2)\n",
        "print(f'Argmax shape: {predicted_label_ids.shape}')\n",
        "\n",
        "# Eliminate axis 0, sentences.\n",
        "predicted_label_ids = np.concatenate(predicted_label_ids, axis=0)\n",
        "all_true_labels = np.concatenate(all_true_labels, axis=0)\n",
        "\n",
        "print(f'Flattened: {predicted_label_ids.shape}')\n",
        "print(f'Ground truth: {all_true_labels.shape}')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predictions shape: (2443, 50, 26)\n",
            "Argmax shape: (2443, 50)\n",
            "Flattened: (122150,)\n",
            "Ground truth: (122150,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHpkP84SAiE7",
        "colab_type": "text"
      },
      "source": [
        "## Filter\n",
        "* Strip out predictions on BERT tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4Twlao9rrYz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "26f771e3-4713-41a3-caa9-bee9cd3e3568"
      },
      "source": [
        "real_token_predictions = []\n",
        "real_token_labels = []\n",
        "\n",
        "for i in range(len(all_true_labels)):\n",
        "    if not all_true_labels[i] == NULL_LABEL_ID:\n",
        "        real_token_predictions.append(predicted_label_ids[i])\n",
        "        real_token_labels.append(all_true_labels[i])\n",
        "\n",
        "print(f'Before filtering : {len(all_true_labels):,}')\n",
        "print(f'After filtering  : {len(real_token_labels):,}')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before filtering : 122,150\n",
            "After filtering  : 24,686\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jjvn0ZdryUI",
        "colab_type": "text"
      },
      "source": [
        "## Score\n",
        "* MIT Movie Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbLpUCsDsMGZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "3ede5cee-ccd3-4e2a-9e57-5e4613c28ef1"
      },
      "source": [
        "ivd = {v: k for k, v in label2id.items()}\n",
        "friendly_labels = [ivd[i] for i in range(len(ivd))]\n",
        "print(classification_report(real_token_labels, real_token_predictions, \n",
        "                            target_names=friendly_labels, zero_division=False))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                   precision    recall  f1-score   support\n",
            "\n",
            "       B-DIRECTOR       0.94      0.86      0.90       456\n",
            "           B-YEAR       0.95      0.95      0.95       720\n",
            "          B-GENRE       0.94      0.97      0.95      1117\n",
            "        B-TRAILER       0.82      0.90      0.86        30\n",
            "         I-REVIEW       0.41      0.16      0.23        45\n",
            "           B-PLOT       0.78      0.76      0.77       491\n",
            "          I-ACTOR       0.91      0.95      0.93       862\n",
            "          B-ACTOR       0.91      0.95      0.93       812\n",
            "          B-TITLE       0.88      0.90      0.89       562\n",
            "           I-SONG       0.90      0.79      0.84       119\n",
            "                O       0.97      0.98      0.97     14929\n",
            "         I-RATING       0.96      0.92      0.94       226\n",
            "         B-RATING       0.97      0.97      0.97       500\n",
            "         B-REVIEW       0.48      0.25      0.33        56\n",
            "I-RATINGS_AVERAGE       0.89      0.91      0.90       403\n",
            "B-RATINGS_AVERAGE       0.93      0.92      0.93       451\n",
            "      I-CHARACTER       0.61      0.53      0.57        75\n",
            "        I-TRAILER       0.00      0.00      0.00         8\n",
            "      B-CHARACTER       0.67      0.67      0.67        90\n",
            "          I-GENRE       0.85      0.75      0.80       222\n",
            "           B-SONG       0.70      0.72      0.71        54\n",
            "           I-PLOT       0.81      0.67      0.74       496\n",
            "       I-DIRECTOR       0.94      0.86      0.90       496\n",
            "           I-YEAR       0.97      0.98      0.97       610\n",
            "          I-TITLE       0.91      0.93      0.92       856\n",
            "\n",
            "         accuracy                           0.94     24686\n",
            "        macro avg       0.80      0.77      0.78     24686\n",
            "     weighted avg       0.94      0.94      0.94     24686\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5zWoCUSJZhQ",
        "colab_type": "text"
      },
      "source": [
        "# Closing\n",
        "* Pre-Transformers\n",
        "  * Paper 2018 https://www.aclweb.org/anthology/W18-5711.pdf\n",
        "  * F1-Score : 87.41\n",
        "  * BERT (Base, Uncased) : 94.%\n",
        "* Improving\n",
        "  * Bigger BERT\n",
        "  * More training data\n",
        "  * Speed https://github.com/hanxiao/bert-as-service\n",
        "* NLP Classification Tasks\n",
        "  * Document\n",
        "  * Sentence\n",
        "  * Token\n",
        "* Alternative Libraries\n",
        "  * SpaCy\n",
        "  * Stanford NER \n",
        "* Resources\n",
        "  * http://jalammar.github.io/illustrated-bert/\n",
        "  * https://www.chrismccormick.ai/\n",
        "  * https://github.com/cedrickchee/awesome-bert-nlp\n",
        "  * https://arxiv.org/pdf/1804.00247.pdf\n",
        "  * https://www.tensorflow.org/official_models/fine_tuning_bert\n",
        "  * https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUmf6yKEO6fz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 37,
      "outputs": []
    }
  ]
}